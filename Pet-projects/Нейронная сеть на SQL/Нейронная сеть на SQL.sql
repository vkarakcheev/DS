/*
	Цель проекта – познакомиться с алгоритмом обратного распространения ошибки (backpropagation)
	на примере построения и обучения на SQL простой полносвязной нейронной сети с одним скрытым слоем.
	Проект вдохновлен этой серией видео: https://www.3blue1brown.com/topics/neural-networks.

	Сеть здесь решает задачу multi-label классификации: по значениям продаж за 12 месяцев
	нужно определить, в каких месяцах наблюдался резкий скачок продаж.
	
	Сеть получает на вход 12 значений, соответствующих нормрованным на единицу продажам в каждый месяц.
	На выходе сети вектор из 12 вероятностей скачка продаж в соответствующем месяце.
	В единственном скрытом слое также 12 нейронов.
*/

declare
	    @nx         tinyint = 12     -- размерность входных данных (x)
	   ,@n1         tinyint = 12     -- число нейронов в слое 1
	   ,@n2         tinyint = 12     -- число нейронов в слое 2 и размерность выходных данных (y)
	   ,@wSpread    float   = 1      -- исходный разброс весов  (± это число)
	   ,@bSpread    float   = 1      -- исходный разброс биасов (± это число)
	   ,@epochMax   int     = 100000 -- максимальное число эпох обучения
	   ,@epochErr   int     = 1000   -- число эпох, после каждого из которых считаются ошибки на обучающей и тестовой выборках
	   ,@a          float   = 0.1    -- скорость обучения
	   ,@r          float   = 0      -- коэффициент регуляризации весов
								   
	   ,@INITIALIZE bit     = 1      -- флаги управления скриптом
	   ,@TRAIN      bit     = 1

SET NOCOUNT ON

--———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————
-- Инициализация

-- Инициализируем таблицы весов
IF @INITIALIZE = 1 BEGIN

-- Создаем вспомогательные столбцы с числами от 0 до числа нейронов в каждом слое
drop table if exists #Ix
;with I(i) as ( 
	select cast(0 as tinyint) union all
	select cast(1 as tinyint) + [i]
	from I where i < @nx
) select [i] into #Ix from I

drop table if exists #I1
;with I(i) as ( 
	select cast(0 as tinyint) union all
	select cast(1 as tinyint) + [i]
	from I where i < @n1
) select [i] into #I1 from I

drop table if exists #I2
;with I(i) as ( 
	select cast(0 as tinyint) union all
	select cast(1 as tinyint) + [i]
	from I where i < @n2
) select [i] into #I2 from I

/*
	Дальше инициализируются матрицы весов каждого слоя, 
	которые хранятся как плоские (narrow, stacked) таблицы [ANN_W1] и [ANN_W2],
	состоящие из полей с индексами нейронов текущего и предыдущего слоев и поля с весами
*/

-- Инициализируем матрицу весов слоя 1 (скрытого):
--		iх – индекс входного нейрона
--		i1 – индекс нейрона слоя 1
--		v  – значение веса
truncate table [ANN_W1]
insert into    [ANN_W1] (i1, ix, v)
select I1.i, Ix.i, case
					   when I1.i = 0 then 0e0                                        -- в нулевой строке  (0, …) не будет ничего значащего, она нужна для правильного перемножения матриц при существовании нулевой строки у выходов
					   when Ix.i = 0 then @bSpread*(2e0*rand(checksum(newid()))-1e0) -- в нулевом столбце (…, 0) будут смещения (биасы)
					   else               @wSpread*(2e0*rand(checksum(newid()))-1e0)
				   end
from #I1 I1, #Ix Ix

-- Инициализируем матрицу весов слоя 2 (выходного)
--		i1 – индекс нейрона слоя 1
--		i2 – индекс нейрона слоя 2
--		v  – значение веса
truncate table [ANN_W2]
insert into    [ANN_W2] (i2, i1, v)
select I2.i, I1.i, case
					   when I2.i = 0 then 0e0                                        -- в нулевой строке  (0, …) не будет ничего значащего, она нужна для правильного перемножения матриц при существовании нулевой строки у выходов
					   when I1.i = 0 then @bSpread*(2e0*rand(checksum(newid()))-1e0) -- в нулевом столбце (…, 0) будут смещения (биасы)
					   else               @wSpread*(2e0*rand(checksum(newid()))-1e0)
				   end
from #I2 I2, #I1 I1

drop table if exists #Ix
drop table if exists #I1
drop table if exists #I2

END -- INITIALIZE

--———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————
-- Обучение

IF @TRAIN = 1 BEGIN

-- Очищаем таблицу с историей ошибок
truncate table [ANN_ErrorDynamics]

declare @epoch int = 0                         -- текущая эпоха
	   ,@Nw    int = @n1*(@nx+1) + @n2*(@n1+1) -- количество всех весов (для расчета регуляризующего члена)

WHILE @epoch <= @epochMax BEGIN


/*
	В таблице [ANN_Sets] в плоском (narrow, stacked) виде хранится обучающий и тестовый наборы данных:
		type  – тип набора:   0 = обучающий,   1 = тестовый
		param – тип значения: 0 = входное (x), 1 = выходное (y, label)
		k     – индекс примера (объекта, сэмпла)
		i     – индекс месяца
		v     – значение

	В таблицах [ANN_O1] и [ANN_O2] в плоском (narrow, stacked) виде хранятся текущие значения 
	выходного (Output) сигнала нейронов в соответствующем слое для *всего* набора входных данных:
	каждую эпоху – для обучающего, каждые @epochsED эпох – еще и для тестового (для расчета ошибки на тестовой выборке).

	Функция активации в обоих слоях (скрытом и выходном) – сигмоида (sigmoid).
	Функция ошибки (loss) – среднеквадратическая (Mean Squared Error, MSE).
*/

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-- Обновляем выходы слоев

-- Обновляем выходы слоя 1 (скрытого)
truncate table [ANN_O1]
insert into    [ANN_O1]
select
	  x.[type]
	, x.[k]
	,w1.[i1]
	,   [v] = iif(w1.i1 > 0, 1e0/( 1e0+exp(-sum(w1.v*x.v)) ), 1e0) -- оставляем 1 в нулевой строке для правильного учета биасов
from [ANN_Sets] x
join [ANN_W1]   w1 on w1.ix = x.i
where   x.param = 0 -- входы сети (x)
	and x.type  <= iif(@epoch % @epochErr > 0, 0, 1) -- только обучающая выборка, если не прошли очередные @epochsED эпох, и дополнительно тестовая выборка, если прошли
group by x.[type], x.[k], w1.[i1]

-- Обновляем выходы слоя 2 (выходного)
truncate table [ANN_O2]
insert into    [ANN_O2]
select
	 o1.[type]
	,o1.[k]
	,w2.[i2]
	,   [v] = iif(w2.i2 > 0, 1e0/( 1e0+exp(-sum(w2.v*o1.v)) ), 1e0) -- оставляем 1 в нулевой строке для правильного учета биасов
from [ANN_O1] o1
join [ANN_W2] w2 on w2.i1 = o1.i1
group by o1.[type], o1.[k], w2.[i2]

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-- Обновляем ошибки

-- Обновляем ошибки на обучающей и тестовой выборках каждые @epochsED эпох
-- и сохраняем их значения в таблицу [ANN_ErrorDynamics]
IF @epoch % @epochErr = 0
with e as ( -- ошибка для каждого примера из выборки
	select
		 y.[type]
		,y.[k]
		,  [v] = sum(square(o2.v-y.v)) -- сумма по o2.i2, т.е. по нейронам слоя 2 (выходного)
	from [ANN_O2]   o2
	join [ANN_Sets] y  on y.type = o2.type and y.k = o2.k and y.i = o2.i2
	where y.param = 1 -- истинные выходы сети (y, labels)
	group by y.[type], y.[k]
)
insert [ANN_ErrorDynamics]
select
	 [epoch] = @epoch
	,[type]
	,[error] = avg(v)/2e0 -- итоговая средняя по выборке ошибка (дополнительно делим на 2, чтобы избавиться от двойки в расчете производных)
		       + @r*(select avg(v*v)/2e0 from ( select v from [ANN_W1] where i1 > 0 union all
				                                select v from [ANN_W2] where i2 > 0) x ) -- добавляем к ошибке средний квадрат весов для регуляризации (дополнительно делим на 2, чтобы избавиться от двойки в расчете производных)
from e
group by [type]

-- Условие преждевременного окончания обучения: ошибка на тестовой выборке начинает расти вместо снижения
IF  (select error from [ANN_ErrorDynamics] where type = 1 and epoch = @epoch) >
	(select error from [ANN_ErrorDynamics] where type = 1 and epoch = @epoch-@epochErr)
	or @epoch = @epochMax -- последний проход цикла нужен только для того, чтобы рассчитать финальные выходы и ошибки, поэтому следующий ниже пересчет весов на нем не нужен
	BREAK

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-- Обновляем веса

-- Обновляем частные производные ошибки по выходам слоя 1 (скрытого) o1 для каждого примера из обучающей выборки
drop table if exists #dEdo1 
select
	  y.[k]
	,w2.[i1]
	,   [v] = sum( w2.v*o2.v*(1e0-o2.v)*(o2.v-y.v) ) -- сумма по w2.i2, т.е. по нейронам слоя 2 (выходного)
into #dEdo1
from [ANN_Sets] y  
join [ANN_O2]   o2 on o2.i2 = y.i and o2.k = y.k and o2.type = y.type 
join [ANN_W2]   w2 on w2.i2 = y.i               
where   y.param = 1 -- истинные выходы сети (y, labels)
	and y.type  = 0 -- обучающая выборка
group by y.[k], w2.[i1]

-- Обновляем веса слоя 2 (выходного) w2
;with dEdw2 as ( -- частные производные ошибки по w2 для всей обучающей выборки в среднем
	select
		 o2.[i2]
		,o1.[i1]
		,   [v] = avg( o1.v*o2.v*(1e0-o2.v)*(o2.v-y.v) ) -- усреднение по k, т.е. по обучающей выборке
	from [ANN_Sets] y  
	join [ANN_O2]   o2 on o2.k = y.k and o2.type = y.type and o2.i2 = y.i  
	join [ANN_O1]   o1 on o1.k = y.k and o1.type = y.type
	where   y.param = 1 -- истинные выходы сети (y, labels)
		and y.type  = 0 -- обучающая выборка
	group by o2.[i2], o1.[i1]
)
update [ANN_W2] set v -= @a*(dEdw2.v + @r*w2.v/@Nw) -- добавляем регуляризующий член
from   [ANN_W2] w2
join            dEdw2 
	on  dEdw2.i2 = w2.i2
	and dEdw2.i1 = w2.i1 

-- Обновляем веса слоя 1 (скрытого) w1
;with dEdw1 as ( -- частные производные ошибки по w1 для всей обучающей выборки в среднем
	select
		 o1.[i1]
		,   [ix] = x.i
		,   [v]  = avg( x.v*o1.v*(1e0-o1.v)*dEdo1.v ) -- усреднение по k, т.е. по обучающей выборке
	from [ANN_Sets] x     
	join [ANN_O1]   o1    on    o1.k = x.k and o1.type = x.type 
	join #dEdo1     dEdo1 on dEdo1.k = x.k and dEdo1.i1 = o1.i1 
	where   x.param = 0 -- входы сети (x)
		and x.type  = 0 -- обучающая выборка
	group by o1.[i1], x.[i]
)
update [ANN_W1] set v -= @a*(dEdw1.v + @r*w1.v/@Nw) -- добавляем регуляризующий член
from   [ANN_W1] w1
join            dEdw1
	on  dEdw1.i1 = w1.i1
	and dEdw1.ix = w1.ix
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

set @epoch += 1

END -- WHILE 

drop table if exists #dEdo1 

select [Last epoch] = @epoch -- выводим последнюю эпоху

END -- TRAIN

--———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————
